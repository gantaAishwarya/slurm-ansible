---
- name: Slurm execution hosts
  hosts: ansible_clients
  #gather_facts: no
  become: yes
  roles:
    - role: galaxyproject.slurm
      become: True
 
  pre_tasks:
    - name: Create variable from command
      ansible.builtin.script:
        cmd: example.py
      register: slurm_output
    - name: set parameter values as fact
      set_fact:
        conf: "{{ conf | default({}) | combine ( { item.split(':')[0]: item.split(':')[1] } ) }}"
      with_items: "{{ slurm_output.stdout_lines[1:-1] }}"
    - debug:
        msg: "{{ conf }}" 
  vars:
    #contents: "{{ lookup('file', '/home/test/Documents/Node_config.json') }}"
    s_nodes: "{{ slurm_output.stdout_lines[1:-1] }}"
    #tasks:
    #    - debug: msg="the value of foo.txt is {{ contents }}"
    #slurm_cgroup_config:
      #CgroupMountpoint: "/sys/fs/cgroup"
      #CgroupAutomount: yes
      #ConstrainCores: yes
      #TaskAffinity: no
      #ConstrainRAMSpace: yes
      #ConstrainSwapSpace: no
      #ConstrainDevices: no
      #AllowedRamSpace: 100
      #AllowedSwapSpace: 0
      #MaxRAMPercent: 100
      #MaxSwapPercent: 100
      #MinRAMSpace: 30
    slurm_config:
      AccountingStorageType: "accounting_storage/none"
      ClusterName: patrec-gpu-cluster
      #GresTypes: gpu
      JobAcctGatherType: "jobacct_gather/none"
      MpiDefault: none
      ProctrackType: "proctrack/pgid"
      ReturnToService: 1
      #SchedulerType: "sched/backfill"
      #SelectType: "select/cons_res"
      #SelectTypeParameters: "CR_Core"
      SlurmctldHost: "emmerich"
      SlurmctldLogFile: "/var/log/slurm/slurmctld.log"
      SlurmctldPidFile: "/var/run/slurmctld.pid"
      SlurmdLogFile: "/var/log/slurm/slurmd.log"
      SlurmdPidFile: "/var/run/slurmd.pid"
      SlurmdSpoolDir: "/var/spool/slurmd"
      StateSaveLocation: "/var/spool/slurmctld"
      SwitchType: "switch/none"
      TaskPlugin: "task/affinity"
      #SelectType=select/cons_res
      #TaskPluginParam: Sched
    slurm_create_user: yes
    #slurm_gres_config:
    #  - File: /dev/nvidia[0-3]
    #    Name: gpu
    #    NodeName: gpu[01-10]
    #    Type: tesla
    slurm_munge_key: "/etc/munge/munge.key"
    
    #slurm_nodes:
      #"{{ slurm_nodes }}"
      #output123: "{{slurm_nodes}}"
      #File: yoyo.txt    
     # shell: /usr/sbin/slurmd -C
      #register: node_conf
    slurm_partitions:
      - name: debug
        Default: YES
        MaxTime: UNLIMITED
        Nodes: "ALL"
    slurm_roles: ['exec']
    slurm_user:
      comment: "Slurm Workload Manager"
      gid: 888
      group: slurm
      home: "/var/lib/slurm"
      name: slurm
      shell: "/usr/sbin/nologin"
      uid: 888
